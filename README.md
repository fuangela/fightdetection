# Human Pose Based Fight Detection System

A comprehensive computer vision system for real-time fight detection using pose estimation and machine learning techniques.

## ğŸ¯ Project Overview

This repository contains multiple approaches to fight detection:
- **Projectv4**: Production-ready LSTM-based pose detection (Recommended)
- **Projectv3**: Experimental VLM (Visual Language Model) approach using LLaVA
- **FightDetectionPoseLSTM**: Structured pose-based classification system
- Dataset tools and preprocessing utilities

## ğŸ† Best Pipeline: Projectv4

Projectv4 offers the most robust and efficient fight detection pipeline with:
- Real-time pose estimation using MediaPipe
- LSTM temporal modeling for action classification
- Advanced person tracking and ID assignment
- Production-ready camera integration
- Comprehensive diagnostics and configuration

### Key Features:
- âœ… Real-time webcam processing
- âœ… Multi-person tracking
- âœ… Temporal sequence analysis (60 frames)
- âœ… Configurable confidence thresholds
- âœ… Efficient pose feature extraction (21D vectors)

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Projectv4/                    # Main production pipeline
â”‚   â”œâ”€â”€ camera.py                 # Real-time detection
â”‚   â”œâ”€â”€ cameratesting.py          # Testing utilities
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ diagnostics.py            # Model evaluation
â”‚   â”œâ”€â”€ featureextract.py         # Pose feature engineering
â”‚   â”œâ”€â”€ posedetect.py             # Pose estimation core
â”‚   â”œâ”€â”€ processdata.py            # Data preprocessing
â”‚   â”œâ”€â”€ trainmodel.py             # Model training
â”‚   â””â”€â”€ run_*.py                  # Execution scripts
â”œâ”€â”€ Projectv3/                    # VLM experimental approach
â”‚   â”œâ”€â”€ vlmtry.py                 # LLaVA-based detection
â”‚   â””â”€â”€ LLaVA/                    # VLM implementation
â”œâ”€â”€ FightDetectionPoseLSTM/       # Structured pose analysis
â”‚   â”œâ”€â”€ 1_MultipleVideoFrameExtraction.py
â”‚   â”œâ”€â”€ 2_AngleVectorsCalculation.py
â”‚   â”œâ”€â”€ 3_AngleVectorsAdjustement.py
â”‚   â”œâ”€â”€ 4_1_LSTMClassification.py
â”‚   â””â”€â”€ 4_2_MLClassification.py
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install opencv-python mediapipe tensorflow numpy scikit-learn
```

### Running the System
```bash
cd Projectv4
python camera.py  # Start real-time detection
```

### Training Custom Models
```bash
cd Projectv4
python run_processdata.py  # Prepare training data
python run_trainmodel.py   # Train LSTM model
```

## ğŸ”§ Configuration

Edit `Projectv4/config.py` to customize:
- Detection confidence thresholds
- Sequence length for temporal analysis
- Camera settings and resolution
- Model parameters

## ğŸ“Š Performance

**Projectv4 Benchmarks:**
- **Accuracy**: ~85-90% on test dataset
- **Speed**: 15-30 FPS real-time processing
- **Latency**: <100ms per frame
- **Resource**: CPU-optimized (no GPU required)

## ğŸ”¬ Alternative Approaches

### VLM Approach (Projectv3)
- Uses LLaVA vision-language model
- High accuracy but computationally expensive
- Requires GPU and model server setup
- Better for offline analysis

### Traditional ML (FightDetectionPoseLSTM)
- Structured pipeline with angle-based features
- Good baseline implementation
- Less optimized for real-time use

## ğŸ“ˆ Model Architecture

**Projectv4 Pipeline:**
1. **Pose Detection**: MediaPipe Holistic
2. **Feature Extraction**: 21D vectors (4 angles + 17 velocities)
3. **Temporal Modeling**: LSTM with 60-frame sequences
4. **Classification**: Binary fight/no-fight prediction
5. **Post-processing**: Confidence filtering and smoothing

## ğŸ¥ Supported Input

- Live webcam feed
- Video files (MP4, AVI, etc.)
- Image sequences
- Multiple camera streams

## ğŸ“ Dataset

The system was trained on the fight-detection-surv-dataset containing:
- Fight scenarios: Punching, kicking, wrestling
- No-fight scenarios: Normal activities, conversations
- Various environments and lighting conditions

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- MediaPipe team for pose estimation
- Fight detection surveillance dataset contributors
- LLaVA team for vision-language model research

## ğŸ“ Contact

For questions or collaboration opportunities, please open an issue or contact the maintainers.

---

**Note**: Large model files and datasets are excluded from this repository. Please refer to the documentation for downloading and setting up required models and datasets.
